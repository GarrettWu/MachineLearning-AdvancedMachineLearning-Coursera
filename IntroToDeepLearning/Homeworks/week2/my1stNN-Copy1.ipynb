{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Garrett/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 28, 28) (50000,)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from preprocessed_mnist import load_dataset\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_dataset()\n",
    "print(X_train.shape, y_train.shape)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.imshow(X_train[0], cmap=\"Greys\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_h = img_w = 28             # MNIST images are 28x28\n",
    "img_size = img_size_flat = img_h * img_w  # 28x28=784, the total number of pixels\n",
    "n_classes = 10                 # Number of classes, one class per digit\n",
    "\n",
    "def randomize(x, y):\n",
    "    \"\"\" Randomizes the order of data samples and their corresponding labels\"\"\"\n",
    "    permutation = np.random.permutation(y.shape[0])\n",
    "    shuffled_x = x[permutation, :]\n",
    "    shuffled_y = y[permutation]\n",
    "    return shuffled_x, shuffled_y\n",
    "\n",
    "def get_next_batch(x, y, start, end):\n",
    "    x_batch = x[start:end]\n",
    "    y_batch = y[start:end]\n",
    "    return x_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 0 4 1 9]\n",
      "(50000, 784)\n",
      "(50000, 10)\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "x_train = X_train = X_train.reshape([-1, img_size])\n",
    "X_val = X_val.reshape([-1, img_size])\n",
    "X_test = X_test.reshape([-1, img_size])\n",
    "\n",
    "print(y_train[0:5])\n",
    "def get_one_hot(y):\n",
    "    y = y.astype(int)\n",
    "    y_one_hot = np.zeros([y.shape[0], n_classes])\n",
    "    for i in range(y.shape[0]):\n",
    "        y_one_hot[i, y[i]] = 1\n",
    "    \n",
    "    return y_one_hot\n",
    "\n",
    "y_train = get_one_hot(y_train)\n",
    "y_val = get_one_hot(y_val)\n",
    "y_test = get_one_hot(y_test)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(y_train[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "epochs = 200             # Total number of training epochs\n",
    "batch_size = 100        # Training batch size\n",
    "display_freq = 100      # Frequency of displaying the training results\n",
    "learning_rate = 0.01   # The optimization initial learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "h1 = 200                # Number of units in the first hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight and bais wrappers\n",
    "def weight_variable(name, shape):\n",
    "    \"\"\"\n",
    "    Create a weight variable with appropriate initialization\n",
    "    :param name: weight name\n",
    "    :param shape: weight shape\n",
    "    :return: initialized weight variable\n",
    "    \"\"\"\n",
    "    initer = tf.truncated_normal_initializer(stddev=0.01)\n",
    "    return tf.get_variable('W_' + name,\n",
    "                           dtype=tf.float32,\n",
    "                           shape=shape,\n",
    "                           initializer=initer)\n",
    "\n",
    "def bias_variable(name, shape):\n",
    "    \"\"\"\n",
    "    Create a bias variable with appropriate initialization\n",
    "    :param name: bias variable name\n",
    "    :param shape: bias variable shape\n",
    "    :return: initialized bias variable\n",
    "    \"\"\"\n",
    "    initial = tf.constant(0., shape=shape, dtype=tf.float32)\n",
    "    return tf.get_variable('b_' + name,\n",
    "                           dtype=tf.float32,\n",
    "                           initializer=initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fc_layer(x, num_units, name, use_relu=True):\n",
    "    \"\"\"\n",
    "    Create a fully-connected layer\n",
    "    :param x: input from previous layer\n",
    "    :param num_units: number of hidden units in the fully-connected layer\n",
    "    :param name: layer name\n",
    "    :param use_relu: boolean to add ReLU non-linearity (or not)\n",
    "    :return: The output array\n",
    "    \"\"\"\n",
    "    in_dim = x.get_shape()[1]\n",
    "    W = weight_variable(name, shape=[in_dim, num_units])\n",
    "    b = bias_variable(name, [num_units])\n",
    "    layer = tf.matmul(x, W)\n",
    "    layer += b\n",
    "    if use_relu:\n",
    "        layer = tf.sigmoid(layer)\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the graph for the linear model\n",
    "# Placeholders for inputs (x) and outputs(y)\n",
    "x = tf.placeholder(tf.float32, shape=[None, img_size_flat], name='X')\n",
    "y = tf.placeholder(tf.float32, shape=[None, n_classes], name='Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc1 = fc_layer(x, h1, 'FC1', use_relu=True)\n",
    "output_logits = fc_layer(fc1, n_classes, 'OUT', use_relu=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-11-f6c908eebc7b>:1: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=output_logits), name='loss')\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, name='Adam').minimize(loss)\n",
    "# _, accuracy = tf.metrics.accuracy(labels=tf.argmax(y, 1), predictions=tf.argmax(output_logits, 1), name='accuracy')\n",
    "\n",
    "# # Network predictions\n",
    "# cls_prediction = tf.argmax(output_logits, axis=1, name='predictions')\n",
    "\n",
    "# # Define the loss function, optimizer, and accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(output_logits, 1), tf.argmax(y, 1), name='correct_pred')\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch   0, loss 2.56, accuracy 21.7%\n",
      "epoch   1, loss 2.23, accuracy 23.0%\n",
      "epoch   2, loss 2.19, accuracy 15.1%\n",
      "epoch   3, loss 2.17, accuracy 25.9%\n",
      "epoch   4, loss 2.11, accuracy 36.9%\n",
      "epoch   5, loss 2.01, accuracy 42.6%\n",
      "epoch   6, loss 1.89, accuracy 43.7%\n",
      "epoch   7, loss 1.74, accuracy 47.0%\n",
      "epoch   8, loss 1.59, accuracy 53.5%\n",
      "epoch   9, loss 1.45, accuracy 59.6%\n",
      "epoch  10, loss 1.32, accuracy 63.9%\n",
      "epoch  11, loss 1.20, accuracy 66.9%\n",
      "epoch  12, loss 1.10, accuracy 69.7%\n",
      "epoch  13, loss 1.01, accuracy 73.2%\n",
      "epoch  14, loss 0.92, accuracy 76.8%\n",
      "epoch  15, loss 0.84, accuracy 78.8%\n",
      "epoch  16, loss 0.79, accuracy 79.5%\n",
      "epoch  17, loss 0.73, accuracy 80.1%\n",
      "epoch  18, loss 0.68, accuracy 80.8%\n",
      "epoch  19, loss 0.64, accuracy 81.8%\n",
      "epoch  20, loss 0.60, accuracy 82.5%\n",
      "epoch  21, loss 0.57, accuracy 83.4%\n",
      "epoch  22, loss 0.54, accuracy 84.4%\n",
      "epoch  23, loss 0.51, accuracy 85.4%\n",
      "epoch  24, loss 0.48, accuracy 86.4%\n",
      "epoch  25, loss 0.46, accuracy 87.1%\n",
      "epoch  26, loss 0.44, accuracy 87.6%\n",
      "epoch  27, loss 0.42, accuracy 88.1%\n",
      "epoch  28, loss 0.40, accuracy 88.6%\n",
      "epoch  29, loss 0.39, accuracy 88.9%\n",
      "epoch  30, loss 0.37, accuracy 89.2%\n",
      "epoch  31, loss 0.36, accuracy 89.6%\n",
      "epoch  32, loss 0.35, accuracy 89.9%\n",
      "epoch  33, loss 0.34, accuracy 90.1%\n",
      "epoch  34, loss 0.33, accuracy 90.3%\n",
      "epoch  35, loss 0.32, accuracy 90.5%\n",
      "epoch  36, loss 0.32, accuracy 90.7%\n",
      "epoch  37, loss 0.31, accuracy 90.9%\n",
      "epoch  38, loss 0.30, accuracy 91.0%\n",
      "epoch  39, loss 0.30, accuracy 91.2%\n",
      "epoch  40, loss 0.29, accuracy 91.4%\n",
      "epoch  41, loss 0.28, accuracy 91.6%\n",
      "epoch  42, loss 0.28, accuracy 91.8%\n",
      "epoch  43, loss 0.27, accuracy 91.9%\n",
      "epoch  44, loss 0.27, accuracy 92.0%\n",
      "epoch  45, loss 0.26, accuracy 92.1%\n",
      "epoch  46, loss 0.26, accuracy 92.3%\n",
      "epoch  47, loss 0.26, accuracy 92.4%\n",
      "epoch  48, loss 0.25, accuracy 92.5%\n",
      "epoch  49, loss 0.25, accuracy 92.7%\n",
      "epoch  50, loss 0.24, accuracy 92.8%\n",
      "epoch  51, loss 0.24, accuracy 93.0%\n",
      "epoch  52, loss 0.24, accuracy 93.1%\n",
      "epoch  53, loss 0.23, accuracy 93.2%\n",
      "epoch  54, loss 0.23, accuracy 93.3%\n",
      "epoch  55, loss 0.23, accuracy 93.4%\n",
      "epoch  56, loss 0.22, accuracy 93.5%\n",
      "epoch  57, loss 0.22, accuracy 93.6%\n",
      "epoch  58, loss 0.22, accuracy 93.7%\n",
      "epoch  59, loss 0.22, accuracy 93.8%\n",
      "epoch  60, loss 0.21, accuracy 93.9%\n",
      "epoch  61, loss 0.21, accuracy 94.0%\n",
      "epoch  62, loss 0.21, accuracy 94.1%\n",
      "epoch  63, loss 0.21, accuracy 94.1%\n",
      "epoch  64, loss 0.20, accuracy 94.2%\n",
      "epoch  65, loss 0.20, accuracy 94.2%\n",
      "epoch  66, loss 0.20, accuracy 94.3%\n",
      "epoch  67, loss 0.20, accuracy 94.4%\n",
      "epoch  68, loss 0.20, accuracy 94.4%\n",
      "epoch  69, loss 0.19, accuracy 94.5%\n",
      "epoch  70, loss 0.19, accuracy 94.6%\n",
      "epoch  71, loss 0.19, accuracy 94.6%\n",
      "epoch  72, loss 0.19, accuracy 94.7%\n",
      "epoch  73, loss 0.19, accuracy 94.8%\n",
      "epoch  74, loss 0.18, accuracy 94.8%\n",
      "epoch  75, loss 0.18, accuracy 94.9%\n",
      "epoch  76, loss 0.18, accuracy 94.9%\n",
      "epoch  77, loss 0.18, accuracy 95.0%\n",
      "epoch  78, loss 0.18, accuracy 95.0%\n",
      "epoch  79, loss 0.17, accuracy 95.0%\n",
      "epoch  80, loss 0.17, accuracy 95.1%\n",
      "epoch  81, loss 0.17, accuracy 95.1%\n",
      "epoch  82, loss 0.17, accuracy 95.2%\n",
      "epoch  83, loss 0.17, accuracy 95.2%\n",
      "epoch  84, loss 0.17, accuracy 95.3%\n",
      "epoch  85, loss 0.17, accuracy 95.3%\n",
      "epoch  86, loss 0.16, accuracy 95.4%\n",
      "epoch  87, loss 0.16, accuracy 95.4%\n",
      "epoch  88, loss 0.16, accuracy 95.5%\n",
      "epoch  89, loss 0.16, accuracy 95.5%\n",
      "epoch  90, loss 0.16, accuracy 95.5%\n",
      "epoch  91, loss 0.16, accuracy 95.6%\n",
      "epoch  92, loss 0.16, accuracy 95.6%\n",
      "epoch  93, loss 0.15, accuracy 95.6%\n",
      "epoch  94, loss 0.15, accuracy 95.7%\n",
      "epoch  95, loss 0.15, accuracy 95.7%\n",
      "epoch  96, loss 0.15, accuracy 95.8%\n",
      "epoch  97, loss 0.15, accuracy 95.8%\n",
      "epoch  98, loss 0.15, accuracy 95.9%\n",
      "epoch  99, loss 0.15, accuracy 95.9%\n",
      "epoch 100, loss 0.14, accuracy 95.9%\n",
      "epoch 101, loss 0.14, accuracy 96.0%\n",
      "epoch 102, loss 0.14, accuracy 96.0%\n",
      "epoch 103, loss 0.14, accuracy 96.1%\n",
      "epoch 104, loss 0.14, accuracy 96.1%\n",
      "epoch 105, loss 0.14, accuracy 96.1%\n",
      "epoch 106, loss 0.14, accuracy 96.2%\n",
      "epoch 107, loss 0.14, accuracy 96.2%\n",
      "epoch 108, loss 0.14, accuracy 96.2%\n",
      "epoch 109, loss 0.13, accuracy 96.3%\n",
      "epoch 110, loss 0.13, accuracy 96.3%\n",
      "epoch 111, loss 0.13, accuracy 96.3%\n",
      "epoch 112, loss 0.13, accuracy 96.4%\n",
      "epoch 113, loss 0.13, accuracy 96.4%\n",
      "epoch 114, loss 0.13, accuracy 96.4%\n",
      "epoch 115, loss 0.13, accuracy 96.5%\n",
      "epoch 116, loss 0.13, accuracy 96.5%\n",
      "epoch 117, loss 0.13, accuracy 96.5%\n",
      "epoch 118, loss 0.13, accuracy 96.6%\n",
      "epoch 119, loss 0.12, accuracy 96.6%\n",
      "epoch 120, loss 0.12, accuracy 96.6%\n",
      "epoch 121, loss 0.12, accuracy 96.7%\n",
      "epoch 122, loss 0.12, accuracy 96.7%\n",
      "epoch 123, loss 0.12, accuracy 96.7%\n",
      "epoch 124, loss 0.12, accuracy 96.8%\n",
      "epoch 125, loss 0.12, accuracy 96.8%\n",
      "epoch 126, loss 0.12, accuracy 96.8%\n",
      "epoch 127, loss 0.12, accuracy 96.9%\n",
      "epoch 128, loss 0.12, accuracy 96.9%\n",
      "epoch 129, loss 0.11, accuracy 96.9%\n",
      "epoch 130, loss 0.11, accuracy 96.9%\n",
      "epoch 131, loss 0.11, accuracy 97.0%\n",
      "epoch 132, loss 0.11, accuracy 97.0%\n",
      "epoch 133, loss 0.11, accuracy 97.0%\n",
      "epoch 134, loss 0.11, accuracy 97.0%\n",
      "epoch 135, loss 0.11, accuracy 97.1%\n",
      "epoch 136, loss 0.11, accuracy 97.1%\n",
      "epoch 137, loss 0.11, accuracy 97.1%\n",
      "epoch 138, loss 0.11, accuracy 97.1%\n",
      "epoch 139, loss 0.11, accuracy 97.2%\n",
      "epoch 140, loss 0.11, accuracy 97.2%\n",
      "epoch 141, loss 0.10, accuracy 97.2%\n",
      "epoch 142, loss 0.10, accuracy 97.2%\n",
      "epoch 143, loss 0.10, accuracy 97.3%\n",
      "epoch 144, loss 0.10, accuracy 97.3%\n",
      "epoch 145, loss 0.10, accuracy 97.3%\n",
      "epoch 146, loss 0.10, accuracy 97.4%\n",
      "epoch 147, loss 0.10, accuracy 97.4%\n",
      "epoch 148, loss 0.10, accuracy 97.4%\n",
      "epoch 149, loss 0.10, accuracy 97.4%\n",
      "epoch 150, loss 0.10, accuracy 97.4%\n",
      "epoch 151, loss 0.10, accuracy 97.4%\n",
      "epoch 152, loss 0.10, accuracy 97.5%\n",
      "epoch 153, loss 0.10, accuracy 97.5%\n",
      "epoch 154, loss 0.10, accuracy 97.5%\n",
      "epoch 155, loss 0.09, accuracy 97.5%\n",
      "epoch 156, loss 0.09, accuracy 97.6%\n",
      "epoch 157, loss 0.09, accuracy 97.6%\n",
      "epoch 158, loss 0.09, accuracy 97.6%\n",
      "epoch 159, loss 0.09, accuracy 97.6%\n",
      "epoch 160, loss 0.09, accuracy 97.6%\n",
      "epoch 161, loss 0.09, accuracy 97.7%\n",
      "epoch 162, loss 0.09, accuracy 97.7%\n",
      "epoch 163, loss 0.09, accuracy 97.7%\n",
      "epoch 164, loss 0.09, accuracy 97.7%\n",
      "epoch 165, loss 0.09, accuracy 97.7%\n",
      "epoch 166, loss 0.09, accuracy 97.8%\n",
      "epoch 167, loss 0.09, accuracy 97.8%\n",
      "epoch 168, loss 0.09, accuracy 97.8%\n",
      "epoch 169, loss 0.09, accuracy 97.8%\n",
      "epoch 170, loss 0.09, accuracy 97.9%\n",
      "epoch 171, loss 0.08, accuracy 97.9%\n",
      "epoch 172, loss 0.08, accuracy 97.9%\n",
      "epoch 173, loss 0.08, accuracy 97.9%\n",
      "epoch 174, loss 0.08, accuracy 97.9%\n",
      "epoch 175, loss 0.08, accuracy 97.9%\n",
      "epoch 176, loss 0.08, accuracy 98.0%\n",
      "epoch 177, loss 0.08, accuracy 98.0%\n",
      "epoch 178, loss 0.08, accuracy 98.0%\n",
      "epoch 179, loss 0.08, accuracy 98.0%\n",
      "epoch 180, loss 0.08, accuracy 98.0%\n",
      "epoch 181, loss 0.08, accuracy 98.0%\n",
      "epoch 182, loss 0.08, accuracy 98.1%\n",
      "epoch 183, loss 0.08, accuracy 98.1%\n",
      "epoch 184, loss 0.08, accuracy 98.1%\n",
      "epoch 185, loss 0.08, accuracy 98.1%\n",
      "epoch 186, loss 0.08, accuracy 98.1%\n",
      "epoch 187, loss 0.08, accuracy 98.1%\n",
      "epoch 188, loss 0.08, accuracy 98.2%\n",
      "epoch 189, loss 0.07, accuracy 98.2%\n",
      "epoch 190, loss 0.07, accuracy 98.2%\n",
      "epoch 191, loss 0.07, accuracy 98.2%\n",
      "epoch 192, loss 0.07, accuracy 98.2%\n",
      "epoch 193, loss 0.07, accuracy 98.2%\n",
      "epoch 194, loss 0.07, accuracy 98.3%\n",
      "epoch 195, loss 0.07, accuracy 98.3%\n",
      "epoch 196, loss 0.07, accuracy 98.3%\n",
      "epoch 197, loss 0.07, accuracy 98.3%\n",
      "epoch 198, loss 0.07, accuracy 98.3%\n",
      "epoch 199, loss 0.07, accuracy 98.3%\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init = tf.group(tf.local_variables_initializer(), tf.global_variables_initializer())\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        feed_dict = {x: X_train, y: y_train}\n",
    "        sess.run(optimizer, feed_dict=feed_dict)\n",
    "        \n",
    "        current_loss = sess.run(loss, feed_dict=feed_dict)\n",
    "        \n",
    "        current_accuracy = sess.run(accuracy, feed_dict=feed_dict)\n",
    "\n",
    "        print(\"epoch {0:3d}, loss {1:.2f}, accuracy {2:.01%}\".format(epoch, current_loss, current_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create the op for initializing all variables\n",
    "# init = tf.global_variables_initializer()\n",
    "\n",
    "# sess = tf.InteractiveSession()\n",
    "# sess.run(init)\n",
    "# global_step = 0\n",
    "# # Number of training iterations in each epoch\n",
    "# num_tr_iter = int(len(y_train) / batch_size)\n",
    "# for epoch in range(epochs):\n",
    "#     print('Training epoch: {}'.format(epoch + 1))\n",
    "#     x_train, y_train = randomize(x_train, y_train)\n",
    "#     for iteration in range(num_tr_iter):\n",
    "#         global_step += 1\n",
    "#         start = iteration * batch_size\n",
    "#         end = (iteration + 1) * batch_size\n",
    "#         x_batch, y_batch = get_next_batch(x_train, y_train, start, end)\n",
    "\n",
    "#         # Run optimization op (backprop)\n",
    "#         feed_dict_batch = {x: x_batch, y: y_batch}\n",
    "#         sess.run(optimizer, feed_dict=feed_dict_batch)\n",
    "\n",
    "#         if iteration % display_freq == 0:\n",
    "#             # Calculate and display the batch loss and accuracy\n",
    "#             loss_batch, acc_batch = sess.run([loss, accuracy],\n",
    "#                                              feed_dict=feed_dict_batch)\n",
    "\n",
    "#             print(\"iter {0:3d}:\\t Loss={1:.2f},\\tTraining Accuracy={2:.01%}\".\n",
    "#                   format(iteration, loss_batch, acc_batch))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
